# ![Universiteti Metropolitan Tirana](https://umt.edu.al/wp-content/uploads/2024/11/Universiteti-Metropolitan-Tirana.webp)  


# âœ… **Week 2: Algorithm Analysis & Problem Solving**  

## **Algorithm Design Techniques**

Algorithm design techniques provide structured approaches to solving computational problems efficiently. Instead of solving problems arbitrarily, these techniques offer systematic methods to break down problems, optimize solutions, and reduce complexity. The choice of technique depends on the nature of the problem, constraints, and available resources. Below are some fundamental algorithm design strategies, along with examples that illustrate their applications.

### ðŸ”¹ Brute Force

Brute force is the most straightforward problem-solving approach, where all possible solutions are systematically explored until the correct or best one is found. It guarantees correctness but is often computationally expensive for large inputs. 

#### **Example: Password Cracking**
In a brute-force attack, every possible password combination is attempted until the correct one is found. If a password consists of four lowercase letters, an attacker would need to check all possible sequences from `aaaa` to `zzzz`.

```
for each possible password:
    if password matches:
        return success
```

#### **Example: Traveling Salesman Problem (TSP)**
Given a set of cities and distances between them, find the shortest route that visits each city exactly once and returns to the starting point. The brute force method evaluates all possible city orders, leading to an exponential time complexity.

### ðŸ”¹ Divide and Conquer

Divide and conquer is a strategy where a problem is recursively divided into smaller subproblems, solved independently, and then combined to get the final result. It is particularly useful for problems that can be broken down into similar smaller versions of themselves.

#### **Example: Merge Sort**
Sorting a list using merge sort involves dividing the list into two halves, sorting each half recursively, and then merging them back into a sorted list.

```
function merge_sort(arr):
    if length of arr <= 1:
        return arr
    mid = len(arr) // 2
    left = merge_sort(arr[:mid])
    right = merge_sort(arr[mid:])
    return merge(left, right)
```

#### **Example: Binary Search**
Given a sorted array and a target value, binary search finds the target by repeatedly dividing the search interval in half.

```
function binary_search(arr, left, right, target):
    if left > right:
        return -1
    mid = (left + right) // 2
    if arr[mid] == target:
        return mid
    elif arr[mid] > target:
        return binary_search(arr, left, mid - 1, target)
    else:
        return binary_search(arr, mid + 1, right, target)
```

### ðŸ”¹ Greedy Algorithms

Greedy algorithms build up a solution step by step, always choosing the best available option at each step without considering future consequences. While this approach is efficient, it does not always guarantee an optimal solution.

#### **Example: Coin Change Problem**
Given a set of coin denominations, find the minimum number of coins needed to make a given amount. A greedy algorithm selects the largest possible coin at each step.

```
while amount > 0:
    select largest coin possible
    subtract from amount
```

#### **Example: Huffman Coding**
Used in data compression, Huffman coding builds an optimal prefix-free encoding for characters based on their frequencies. The greedy strategy repeatedly merges the two least frequent symbols until only one tree remains.

### ðŸ”¹ Dynamic Programming

Dynamic programming (DP) is used for optimization problems by solving subproblems and storing results to avoid redundant calculations. It is particularly effective for problems with overlapping subproblems and optimal substructure.

#### **Example: Fibonacci Sequence**
A naive recursive approach to computing Fibonacci numbers leads to redundant calculations. DP solves this by storing computed values.

```
memo = {}
def fibonacci(n):
    if n in memo:
        return memo[n]
    if n <= 1:
        return n
    memo[n] = fibonacci(n-1) + fibonacci(n-2)
    return memo[n]
```

#### **Example: 0/1 Knapsack Problem**
Given a set of items with weights and values, determine the maximum value that can be obtained while staying within a weight limit. The DP approach builds a table storing solutions to subproblems to efficiently determine the optimal solution.

## âœ… **Complexity Classes**

P (Polynomial Time) includes problems that can be solved efficiently, meaning in polynomial time (e.g., $O(n)$, $O(n^2)$). Sorting algorithms like Merge Sort and Quick Sort fall into this category.

NP (Nondeterministic Polynomial Time) consists of problems whose solutions can be verified in polynomial time but may not be solvable efficiently. The Traveling Salesman Problem (TSP) is an example, where checking a given route is easy, but finding the shortest route is difficult.

NP-Hard problems are at least as hard as NP problems but are not necessarily in NP. The Halting Problem, which determines whether a program will terminate, is an example.

NP-Complete problems are both in NP and NP-Hard. If any NP-Complete problem is solved in polynomial time, then all NP problems can be solved in polynomial time. The Boolean Satisfiability Problem (SAT) is a classic example.

---

